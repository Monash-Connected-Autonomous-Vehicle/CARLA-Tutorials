{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCAV Carla Tutorial\n",
    "\n",
    "## 1. Background\n",
    "### 1.1. Acknowledgement\n",
    "- The base code of this tutorial is kindly open-sourced by Mr Vadium 7s. \n",
    "- Please visit his github: https://github.com/vadim7s/SelfDrive/tree/master and his Youtube Channel: https://www.youtube.com/@carlasimulator8782 \n",
    "### 1.2. Carla Documentation\n",
    "- Carla Official Documentation: https://carla.readthedocs.io/en/latest/python_api/\n",
    "- For any further questions please contact: yide.tao@monash.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports\n",
    "import carla #the sim library itself\n",
    "import cv2 #to work with images from cameras\n",
    "import numpy as np #in this example to change image representation - re-shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up the World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the sim \n",
    "client = carla.Client('localhost', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Reset the World\n",
    "\n",
    "- The full map name indicate the full path to the map: 'Carla/Maps/Town10HD_Opt'\n",
    "- While the load_world function only takes in Map Name like \"Town10HD_Opt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the world\n",
    "full_map_name = client.get_world().get_map().name\n",
    "current_map = full_map_name.split(\"/\")[-1]\n",
    "world = client.load_world(current_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `spawn_points` is a list of [`carla.Transform`](https://carla.readthedocs.io/en/0.9.8/python_api/#carla.Transform) objects.\n",
    "\n",
    "- To retrieve the `(x, y, z)` coordinates of a spawn point, access the `.location` attribute of the `Transform` object.  \n",
    "  For example, for the first spawn point:\n",
    "\n",
    "  ```python\n",
    "  (spawn_points[0].location.x, \n",
    "   spawn_points[0].location.y, \n",
    "   spawn_points[0].location.z)\n",
    "    ```\n",
    "\n",
    "- These spawnpoints are automatically defined by the OpenDRIVE Files associated with the CARLA World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define environment/world and get possible places to spawn a car\n",
    "spawn_points = world.get_map().get_spawn_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 106.42, y: -12.71, z: 0.60\n"
     ]
    }
   ],
   "source": [
    "# print spawn point\n",
    "def print_spawn_point_xyz(spawn_point):\n",
    "    \"\"\"\n",
    "    Prints the x, y, z coordinates of a given carla.Transform spawn point.\n",
    "\n",
    "    Parameters:\n",
    "    spawn_point (carla.Transform): A transform object with a location attribute.\n",
    "    \"\"\"\n",
    "    x = spawn_point.location.x\n",
    "    y = spawn_point.location.y\n",
    "    z = spawn_point.location.z\n",
    "    print(f\"x: {x:.2f}, y: {y:.2f}, z: {z:.2f}\")\n",
    "\n",
    "print_spawn_point_xyz(spawn_points[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vehicle and Camera\n",
    "\n",
    "### 3.1. Generate Vehicle\n",
    "- Different vehicles can be found here: https://carla.readthedocs.io/en/latest/catalogue_vehicles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for a blueprint of Mini car\n",
    "vehicle_bp = world.get_blueprint_library().filter('*mini*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spawn a car in a random location\n",
    "start_point = spawn_points[0]\n",
    "vehicle = world.try_spawn_actor(vehicle_bp[0], start_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Available Transformation:\n",
    "```python \n",
    "# Define the Position and Orientation\n",
    "position = carla.Location(x=-48.7, y=24.8, z=1.7)\n",
    "orientation = carla.Rotation(pitch=-13.4, yaw=-75.7, roll=0.0)\n",
    "transformation = carla.Transform(position, orientation)\n",
    "\n",
    "# Set Transformation\n",
    "x.set_transform(transformation)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move simulator view to the car\n",
    "spectator = world.get_spectator()\n",
    "start_point.location.z = start_point.location.z+1 #start_point was used to spawn the car but we move 1m up to avoid being on the floor\n",
    "spectator.set_transform(start_point) #move the camera to the vehicle location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carla Traffic Manager offers a set of pre-built commands for managing traffics and vehicle motion: https://carla.readthedocs.io/en/latest/adv_traffic_manager/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send the car off on autopilot - this will leave the spectator\n",
    "vehicle.set_autopilot(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sensors and Blue Prints\n",
    "\n",
    "- Carla Blueprint Library offers a set of complete props: https://carla.readthedocs.io/en/latest/bp_library/, which in general can be categorised into:\n",
    "    1. controller\n",
    "    2. sensor\n",
    "    3. static\n",
    "    4. vehicle\n",
    "    5. walker\n",
    "\n",
    "- Carla Actor Library offers a set of convinient tools: https://carla.readthedocs.io/en/latest/core_actors/?utm_source=chatgpt.com\n",
    "    - The attachment can be attach_to any Carla Object and the AttachmentType can also be varied. \n",
    "\n",
    "- The code below will generate a new Actor: \"Camera\" which follows vehicle around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting RGB Camera - this follow the approach explained in a Carla video\n",
    "# link: https://www.youtube.com/watch?v=om8klsBj4rc&t=1184s\n",
    "\n",
    "#camera mount offset on the car - you can tweak these to each car to avoid any parts of the car being in the view\n",
    "CAMERA_POS_Z = 1.6 #this means 1.6m up from the ground\n",
    "CAMERA_POS_X = 0.9 #this is 0.9m forward\n",
    "\n",
    "camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "camera_bp.set_attribute('image_size_x', '640') # this ratio works in CARLA 9.14 on Windows\n",
    "camera_bp.set_attribute('image_size_y', '360')\n",
    "\n",
    "camera_init_trans = carla.Transform(carla.Location(z=CAMERA_POS_Z,x=CAMERA_POS_X))\n",
    "#this creates the camera in the sim\n",
    "camera = world.spawn_actor(camera_bp,camera_init_trans,attach_to=vehicle)\n",
    "\n",
    "def camera_callback(image,data_dict):\n",
    "    # data_dict is used for image access outside of the image. \n",
    "    data_dict['image'] = np.reshape(np.copy(image.raw_data),(image.height,image.width,4))\n",
    "    # In the callback function, you can also save image to disk by:\n",
    "    # data_dict['image'].save_to_disk('out/%06d.png' % image.frame)\n",
    "\n",
    "image_w = camera_bp.get_attribute('image_size_x').as_int()\n",
    "image_h = camera_bp.get_attribute('image_size_y').as_int()\n",
    "\n",
    "camera_data = {'image': np.zeros((image_h,image_w,4))}\n",
    "# this actually opens a live stream from the camera\n",
    "camera.listen(lambda image: camera_callback(image,camera_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize the live-stream video of the actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "      \n",
    "    # Dispaly with imshow\n",
    "    cv2.imshow('All cameras',camera_data['image'])\n",
    "    \n",
    "    # Break loop if user presses q\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab a snapshot from the camera an show in a pop-up window\n",
    "img = camera_data['image']\n",
    "cv2.imshow('RGB Camera',img)\n",
    "cv2.destroyAllWindows()  # Close all OpenCV windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up after yourself\n",
    "camera.stop() # this is the opposite of camera.listen\n",
    "for actor in world.get_actors().filter('*vehicle*'):\n",
    "    actor.destroy()\n",
    "for sensor in world.get_actors().filter('*sensor*'):\n",
    "    sensor.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Additional Sensors\n",
    "\n",
    "Source: Using the official video: https://www.youtube.com/watch?v=om8klsBj4rc&t=1184s\n",
    "\n",
    "Available Sensors in Carla: \n",
    "1. Cameras\n",
    "    - RGB, Semantic, Optical Flow, Monocular Depth, Dynamic Vision\n",
    "    ```python \n",
    "    # Normal camera. \n",
    "    def rgb_callback(image,data_dict):\n",
    "        data_dict['rgb_image'] = np.reshape(np.copy(image.raw_data),(image.height,image.width,4))\n",
    "\n",
    "    # Semantic Segmentation camera\n",
    "    def sem_callback(image,data_dict):\n",
    "        image.convert(carla.ColorConvert.CityScapesPalette)\n",
    "        data_dict['sem_image'] = np.reshape(np.copy(image.raw_data),(image.height,image.width,4))\n",
    "\n",
    "    # Instance Segmentation camera\n",
    "    def inst_callback(image,data_dict):\n",
    "        data_dict['inst_image'] = np.reshape(np.copy(image.raw_data),(image.height,image.width,4))\n",
    "\n",
    "    # Depth Camera\n",
    "    def depth_callback(image,data_dict):\n",
    "        image.convert(carla.ColorConvert.LogarithmicDepth)\n",
    "        data_dict['depth_image'] = np.reshape(np.copy(image.raw_data),(image.height,image.width,4))\n",
    "\n",
    "    # Optical Flow\n",
    "    def opt_callback(data, data_dict):\n",
    "        image = data.get_color_coded_flow()\n",
    "        img = np.reshape(np.copy(image,raw_data), (image.height, image.width, 4))\n",
    "        img[:, :, 3] = 255\n",
    "        data_dict['opt_image'] = img\n",
    "    \n",
    "    # Dynamic Vision\n",
    "    def dvs_callback(data, data_dict):\n",
    "        dvs_events = np.frombuffer(data.raw_data, dtype = np.dtype([\n",
    "            ('x', np.uint16), ('y', np.uint16), ('t', np.uint64), ('pol', np.bool)\n",
    "        ]))\n",
    "        data_dict['dvs_image'] = np.zeros((data.height, data.width, 4), dtype=np.uint8)\n",
    "        dvs_img = np.zeros((data.height, data.width, 3), dtype = np.uint8)\n",
    "        dvs_img[dvs_events[:]['y'], dvs_events[:]['x'], dvs_events[:]['pol']*2]=255\n",
    "        data_dict['dvs_image'][:, :, 0:3] = dvs_image\n",
    "    ```\n",
    "2. LIDAR\n",
    "    - LiDAR visualization can be done using Open3D\n",
    "3. RADAR\n",
    "4. Inertial Measurement\n",
    "5. Collision/obstacle detection (Event Based Sensor)\n",
    "6. Lane Invasion (Event Based Sensor)\n",
    "7. Satellite Location"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
